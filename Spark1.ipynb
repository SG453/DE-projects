{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b9726f9-8774-48ad-a6e1-00ef2637781e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Optimizing Data Performance: Leveraging Infer Schema and Schema Files in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1ea2c5-54d9-4089-8eda-04a375061c23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this project, we delve into an exploration of the merits and demerits associated with utilizing Infer Schema. We analyze the optimal scenarios for leveraging Infer Schema, shedding light on the intricacies that transpire behind the scenes when this feature is employed. Additionally, we scrutinize the drawbacks of Infer Schema, particularly its performance implications when handling large files. Furthermore, we propose strategies to mitigate performance issues encountered while utilizing Infer Schema in such scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb0cf446-30e6-4d44-9dae-81defd622a25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Infer Schema\n",
    "Infer Schema is a feature commonly found in data processing frameworks and tools like Databricks that allows users to automatically deduce the schema or structure of their data without explicitly specifying it. \n",
    "Here's why it's beneficial:\n",
    "\n",
    "- Automatic Schema Deduction: Infer Schema analyzes the data and determines the data types and structure of each column based on the actual data content. This eliminates the need for manual schema definition, saving time and effort.\n",
    "- Flexibility and Agility: Especially in scenarios where the structure of the data may evolve over time, Infer Schema provides flexibility. As new columns or changes in data types occur, the schema can dynamically adjust without manual intervention.\n",
    "- Reduced Error-Prone Manual Work: Manual schema definition can be error-prone, especially for large datasets with numerous columns. Infer Schema helps avoid mistakes that could lead to incorrect data processing.\n",
    "- Ease of Onboarding: For new datasets or when working with unfamiliar data sources, Infer Schema provides a quick way to understand the structure of the data without requiring prior knowledge or documentation.\n",
    "- Streamlined Development Process: By eliminating the need to define schemas manually, developers and data engineers can focus more on data analysis, transformation, and building pipelines, accelerating the development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47a78753-342e-4bf2-b4b4-519e9c4eae2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When loading data without a predefined schema, InferSchema performs a scan of the dataset to automatically infer the structure and data types of each column. This process typically involves sampling a portion of the data to make initial assumptions about the data types, lengths, and formats present within the dataset. Here's what happens behind the scenes:  \n",
    "**Data Sampling:** InferSchema initially samples a portion of the dataset to analyze the values and their characteristics. This sampling process helps in making preliminary guesses about the data types and structure.  \n",
    "**Type Inference:** Based on the sampled data, InferSchema infers the data types for each column. For example, it might infer that a column predominantly contains numeric values, so it assigns a numeric data type accordingly.  \n",
    "**Schema Generation:** After inferring the data types, InferSchema generates a schema definition for the dataset. This schema includes the inferred data types and any additional metadata such as column names.  \n",
    "\n",
    "\n",
    "While InferSchema offers convenience and flexibility, it can encounter challenges, especially when dealing with very large datasets:  \n",
    "**Performance Overhead:** In the case of large datasets, the process of inferring the schema can be computationally expensive and time-consuming. This overhead increases proportionally with the size of the dataset, potentially leading to longer data loading times and increased resource consumption.  \n",
    "**Memory Consumption:** When processing large datasets, InferSchema may need to load significant portions of the data into memory for sampling and analysis. This can strain system resources, particularly in environments with limited memory capacity, potentially leading to performance degradation or out-of-memory errors.  \n",
    "**Accuracy Limitations:** InferSchema's accuracy heavily depends on the quality and representativeness of the sampled data. With very large datasets, sampling a small portion may not accurately capture the diversity and complexity of the entire dataset, leading to potential inaccuracies in schema inference.  \n",
    "\n",
    "\n",
    "To mitigate these issues when working with large datasets, it's advisable to consider alternative approaches such as providing a predefined schema file. This allows for more efficient and accurate schema definition without the need for costly schema inference operations, thereby improving overall performance and reliability, particularly in resource-constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4008887a-167e-47be-bc2b-024a312a722b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For this demo we are going to use the one of databricks dataset.   \n",
    "\n",
    "#### Dataset details\n",
    "- dataset Name: sai-summit-2019-sf\n",
    "- Folder Path: dbfs:/databricks-datasets/sai-summit-2019-sf/\n",
    "- Total File(s) Size (GB): 1.06\n",
    "- Total File Count: 3\n",
    "- File Types and Counts: {'md': 1, 'csv': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10484b42-4d31-42fe-80d2-a62f124a566a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### creating temmporary view using inferSchema on a 1GB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fc2f21-aebb-49fc-8875-f7fb683af02b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create or replace temporary view tableA \n",
    "using csv \n",
    "options \n",
    "(\n",
    "  path = \"/databricks-datasets/sai-summit-2019-sf/*.csv\", \n",
    "  header = True, \n",
    "  inferSchema = True, \n",
    "  sep = ','\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80891bde-c6eb-4c35-8aa7-0777e480b204",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It took 42.13 seconds to create the temporary view tableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379bae19-57a0-4c94-8c51-03c1a5bfec5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>4380660</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4380660
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select count(1) from tableA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c946a4b7-3ac8-46b1-9274-9102c578d89c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### creating a temporary view using inferSchema using schemaFile on the same 1GB File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6069a77-3773-492a-a295-eaf2e48cdc72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the mapping of Spark data types to string representations\n",
    "datatype_mapping = {\n",
    "    StringType(): \"string\", \n",
    "    IntegerType(): \"int\",\n",
    "    LongType(): \"long\",\n",
    "    DoubleType(): \"double\",\n",
    "    FloatType(): \"float\",\n",
    "    BooleanType(): \"boolean\",\n",
    "    TimestampType(): \"timestamp\",\n",
    "    DateType(): \"date\"\n",
    "}\n",
    "\n",
    "# Read the data with inferSchema=True\n",
    "df = spark.read.option(\"samplingRatio\", 0.01).csv(\"/databricks-datasets/sai-summit-2019-sf/*.csv\", inferSchema=True, header=True, sep=',')\n",
    "\n",
    "# Create the schema string\n",
    "schema_string = \", \".join([f\"{field.name} {datatype_mapping.get(field.dataType, 'string')}\" for field in df.schema.fields])\n",
    "\n",
    "# Create or replace temporary view with the inferred schema\n",
    "df.createOrReplaceTempView(\"tableB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "911355f8-cd76-4443-9956-526051a8f895",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It took only 11.43 seconds when compared to the first result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8daf0643-a910-4691-91ba-dae7180763ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>4380660</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4380660
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select count(1) from tableB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ceb05c3-6d85-4b7f-9e70-57b4fcb30d31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have increased the performance by **72.86%** with the help of schemaFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888056cb-31a9-402e-b0c3-1f0c4bb8db0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
